{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","mount_file_id":"19N4WjDf-Ws72mPbtLEjcnkOqKd6fVCzF","authorship_tag":"ABX9TyOx+dMUU+mqwZd57s//OM4q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5GlAGQZTTzrV","executionInfo":{"status":"ok","timestamp":1692982730882,"user_tz":-120,"elapsed":9347,"user":{"displayName":"gabriele morelli","userId":"10377196500982764207"}},"outputId":"80c606bd-8220-444b-af2c-e3940a90829f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQadFhylQhbo","colab":{"base_uri":"https://localhost:8080/","height":640},"outputId":"6ef701ed-753e-4260-d83b-b84234b5a411","executionInfo":{"status":"error","timestamp":1690928500019,"user_tz":-120,"elapsed":97326,"user":{"displayName":"gabriele morelli","userId":"10377196500982764207"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-2704b61ca7ad>:22: DtypeWarning: Columns (3,4,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  temp_df = pd.read_csv(os.path.join(folder_path, filename))\n"]},{"output_type":"stream","name":"stdout","text":["dataset parziale numero  460  : 1064670\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-2704b61ca7ad>:22: DtypeWarning: Columns (3,4,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  temp_df = pd.read_csv(os.path.join(folder_path, filename))\n"]},{"output_type":"stream","name":"stdout","text":["dataset parziale numero  461  : 2127155\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-2704b61ca7ad>:22: DtypeWarning: Columns (3,4,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  temp_df = pd.read_csv(os.path.join(folder_path, filename))\n"]},{"output_type":"stream","name":"stdout","text":["dataset parziale numero  462  : 3192568\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-2704b61ca7ad>:22: DtypeWarning: Columns (3,4,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  temp_df = pd.read_csv(os.path.join(folder_path, filename))\n"]},{"output_type":"stream","name":"stdout","text":["dataset parziale numero  463  : 4257151\n"]},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mConnectionAbortedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mConnectionAbortedError\u001b[0m: [Errno 103] Software caused connection abort","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-2704b61ca7ad>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'export_log_gmail_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Leggi il file CSV in un dataframe temporaneo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Salta la prima riga (nomi delle colonne) se non si tratta del primo file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n\u001b[1;32m   1781\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1783\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_handles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_handles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_wrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","\n","#//////////////////////////////  INGESTION EMAIL \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n","\n","# Imposta la cartella in cui sono presenti i file CSV\n","folder_path = '/content/gdrive/MyDrive/dataset_email_GMAIL/'\n","\n","# Inizializza il dataframe in cui verranno inseriti i dati\n","df_email_csv = pd.DataFrame()\n","\n","# Itera su tutti i file nella cartella\n","for i, filename in enumerate(os.listdir(folder_path)):\n","\n","    # Ho utilizzato questo per processare più file in contemporanea su diversi notebook\n","    #if (i==2): break\n","    #if( i<460): continue\n","    #if(i==480): break # ricordati che parte da 0 a contare, quindi se scrivi i==50, leggerà fino al dataframe 49\n","\n","    if filename.startswith('export_log_gmail_') and filename.endswith('.csv'):\n","        # metti il file CSV in un dataframe temporaneo\n","        temp_df = pd.read_csv(os.path.join(folder_path, filename))\n","\n","        if df_email_csv.empty:\n","            df_email_csv = temp_df\n","            #  sanity check\n","            print('dataset parziale numero ',i,\" :\", len(df_email_csv))\n","        else:\n","            df_email_csv = pd.concat([df_email_csv, temp_df.iloc[1:]], ignore_index=True)\n","            #  sanity check\n","            print('dataset parziale numero ',i,\" :\", len(df_email_csv))\n","\n","# droppa il vecchio index e ne crea uno \"nuovo\"\n","df_email_csv.reset_index(drop=True, inplace=True)\n","\n","\n","# Stampa il dataframe finale\n","print('dataset completo: ',len(df_email_csv))\n","\n"]},{"cell_type":"code","source":["#%%\n","##////////////////////////////DATASET SENZA GROUP \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n","# Vado a splittare il dataset in email inviate al singolo (\"NOGROUP\") ed email inviate ai gruppi (\"GROUP\")\n","\n","msg_ID_to_drop = df_email_csv.loc[(df_email_csv['source_from_header_address'] == 'Group') | (df_email_csv['source_address'] == 'Group') | (df_email_csv['destination_address'] == 'Group' ) | (df_email_csv['source_service'].isin(['groups', 'mailing-list-server'])) | (df_email_csv['destination_service'].isin(['mailing-list-server'])), 'message_id'].unique()\n","\n","mask = df_email_csv['message_id'].isin(msg_ID_to_drop)\n","df_email_NOGROUP = df_email_csv[~mask]\n","\n","#print('dataset completo NO_group: ',len(df_email_NOGROUP))\n"],"metadata":{"id":"i4rpYp42Qwn3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Funzione per filtrare i record ed ottenere solo le \"email delivered\", cioé quei record che mi permettono di identificare uno scambio tra un mittente ed un destinatario e quinid eliminare le informazioni inerenti tutto il viaggio/percorso delle email\n","def filtraggio(df):\n","\n","    # lato mittente\n","    valid_source_service = ['gmail-ui', 'smtp-inbound', 'smtp-msa', 'smtp-relay']\n","    valid_source_selector = [None, np.nan,  'send', 'gmail-for-work']\n","\n","    \"\"\"\n","    Ho aggiunto: [None, np.nan] nella maschera dei servizi del destinatario perché ci sono delle email che si possono salvare con gmail-ui+send lato mittente e tuttto null lato destinatario\n","    \"\"\"\n","    #lato destinatario\n","    valid_destination_service = [None, np.nan,'gmail-ui', 'smtp-outbound', 'smtp-outbound-to-gmail']\n","\n","    #non mi serve perché tanto con quei service per forza dovrò avere questi selector\n","    #valid_destination_selector = [None, 'gmail-delivery-server', 'google-apps-for-work']\n","\n","    #action type che identificato la ricezione corretta dell'email\n","    valid_action_type = [2, 3, 68]\n","\n","    # Maska to filter out rows with invalid event targets\n","    mask_source_service = df['source_service'].isin(valid_source_service)\n","    mask_source_selector = df['source_selector'].isin(valid_source_selector)\n","    mask_destination_service = df['destination_service'].isin(valid_destination_service)\n","    #mask_destination_selector = df['destination_selector'].isin(valid_destination_selector)\n","    mask_action_type = df['action_type'].isin(valid_action_type)\n","\n","    # Applico la maschera\n","    df_filtered = df[mask_source_service & mask_source_selector & mask_destination_service & mask_action_type]\n","\n","    return df_filtered\n"],"metadata":{"id":"hlw1spgWQzV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#//// ottengo il dataframe filtrato e quello scartato:\n","\n","df_email_NOGROUP_filtrato = filtraggio(df_email_NOGROUP)\n"],"metadata":{"id":"a2vJMlVhQ1zS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ////////////////////////////////////////////// ANALISI MITTENTI MULTIPLI \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n","\n","pivot= pd.pivot_table(df_email_NOGROUP_filtrato, index=[\"message_id\"], values=[\"source_from_header_address\"],   aggfunc=[ pd.Series.nunique], dropna=False)\n","\n","\n","source_id_da_controllare= pivot.loc[pivot[('nunique', 'source_from_header_address')]>1]\n","source_id_da_controllare = source_id_da_controllare.index.get_level_values('message_id')\n","\n","# li scarto\n","df_email_NOGROUP_filtrato = df_email_NOGROUP_filtrato[ ~df_email_NOGROUP_filtrato['message_id'].isin(source_id_da_controllare)]\n"],"metadata":{"id":"pGP8_zjgQ-Zu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#%%\n","\n","##////////////////////////////DATASET CON GROUP \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n","\n","msg_ID_to_drop = df_email_csv.loc[(df_email_csv['source_from_header_address'] == 'Group') | (df_email_csv['source_address'] == 'Group') | (df_email_csv['destination_address'] == 'Group' ) | (df_email_csv['source_service'].isin(['groups', 'mailing-list-server'])) | (df_email_csv['destination_service'].isin(['mailing-list-server'])), 'message_id'].unique()\n","\n","mask = df_email_csv['message_id'].isin(msg_ID_to_drop)\n","df_email_GROUP = df_email_csv[mask]\n","\n"],"metadata":{"id":"qTo-CC3hQ57S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def filtraggio_GROUP(df):\n","\n","    # Define the list of valid event targets\n","    valid_source_service = ['gmail-ui','mailing-list-server', 'smtp-inbound', 'smtp-msa', 'groups']\n","    valid_source_selector = [None, np.nan, 'send', 'groups-ui']\n","\n","\n","    valid_destination_service = ['gmail-ui', 'smtp-outbound', 'smtp-outbound-to-gmail', None, np.nan,]\n","\n","\n","    #valid_destination_selector = [None, 'gmail-delivery-server', np.nan]\n","\n","    valid_action_type = [2, 3, 68]\n","\n","    # Create a mask to filter out rows with invalid event targets\n","    mask_source_service = df['source_service'].isin(valid_source_service)\n","    mask_source_selector = df['source_selector'].isin(valid_source_selector)\n","    mask_destination_service = df['destination_service'].isin(valid_destination_service)\n","    #mask_destination_selector = df['destination_selector'].isin(valid_destination_selector)\n","    mask_action_type = df['action_type'].isin(valid_action_type)\n","\n","\n","    #/////////////////////////////////////////////////////////////////////////////\n","    #togli Group dal mittente e dal destinatario\n","    mask_no_group_in_source_or_dest = ( ~(df['source_from_header_address'].isin(['Group'])) & ~(df['destination_address'].isin(['Group'])))\n","\n","\n","    #togli source=dest perché vuol dire che fa parte della mailing list\n","    #/////////////////////////////////////////////////////////////////////////////\n","\n","\n","    # Filter out invalid rows\n","    df_filtered = df[mask_source_service & mask_source_selector & mask_destination_service & mask_action_type & mask_no_group_in_source_or_dest]\n","\n","\n","    return df_filtered\n"],"metadata":{"id":"gDcT7-KoRKz6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_email_GROUP_filtrato = filtraggio_GROUP(df_email_GROUP)\n"],"metadata":{"id":"Z01oTsWWRMYG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#%%\n","\n","# ////////////////////////////////////////////// ANALISI MITTENTI MULTIPLI GROUP \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n","\n","#io sono interessato che ci sia un unico mittente per email\n","# mittenti per email\n","pivot1= pd.pivot_table(df_email_GROUP_filtrato, index=[\"message_id\"], values=[\"source_from_header_address\"],   aggfunc=[ pd.Series.nunique], dropna=False)\n","\n","source_id_da_controllare= pivot1.loc[pivot1[('nunique', 'source_from_header_address')]>1]\n","source_id_da_controllare = source_id_da_controllare.index.get_level_values('message_id')\n","\n","\n","# SOLUZIONE AI MITTENTI MULTIPLI: droppo le email che hanno più di un mittente\n","df_email_GROUP_filtrato = df_email_GROUP_filtrato[ ~df_email_GROUP_filtrato['message_id'].isin(source_id_da_controllare)]\n"],"metadata":{"id":"Z8bvWXrQRPMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#/////////////////////////////////////////////// UNISCO I DUE DATASET \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n","\n","#Vado ad unire nuovamente i due dataset\n","df_email_completo = pd.concat([df_email_GROUP_filtrato, df_email_NOGROUP_filtrato], ignore_index=True)\n"],"metadata":{"id":"JqQxAT3mRTLH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#DROP DUPLICATES\n","\n","primary_key_cols = ['message_id', 'source_from_header_address', 'destination_address']\n","\n","# Drop duplicate rows based on the primary key columns\n","df_email_completo = df_email_completo.drop_duplicates(subset=primary_key_cols)\n","\n","\n"],"metadata":{"id":"tZjHN3oKRUmw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#////////////////////////// E-MAIL CON SOURCE E DEST UGUALI = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n","\n","\n","# le email con mittente e destinatario uguale non ci interessano, quindi le scarto\n","df_email_completo = df_email_completo[ ~(df_email_completo['source_from_header_address'] == df_email_completo['destination_address']) ]\n"],"metadata":{"id":"QuFxOmRDRZ39"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STATISTICHE ESTRAZIONI\n","\n","print('righe totali del dataset importato: ', len(df_email_csv))\n","print('righe totali del dataset completo dopo il cleaning: ', len(df_email_completo))\n","print()\n","\n","print('email (message_id univoci) totali prima del cleaning: ', df_email_csv[\"message_id\"].nunique())\n","\n","\n","\n","\n","#print(df_email_csv[\"message_id\"].unique())\n","#print(df_email_completo[\"message_id\"].unique())\n","\n","\n","\n","# Drop NaN values from df_email_csv[\"message_id\"]\n","csv_unique_ids = df_email_csv[\"message_id\"].dropna().unique()\n","\n","# Drop NaN values from df_email_completo[\"message_id\"]\n","completo_unique_ids = df_email_completo[\"message_id\"].dropna().unique()\n","\n","# Now csv_unique_ids and completo_unique_ids will contain arrays without NaN values.\n","# You can proceed with finding the unique message_id values that are present in csv_unique_ids but not in completo_unique_ids.\n","missing_ids = pd.Series(list(set(csv_unique_ids) - set(completo_unique_ids)))\n","\n","# Now, missing_ids will contain the unique \"message_id\" values present in df_email_csv but not in df_email_completo.\n","#print(\"message_id scartati: \",missing_ids)\n","\n","print(\"email (message_id univoci) scartate: \", len(set(missing_ids)) )\n","print()\n","\n","print(\"percentuale di email scartate sul dataset completo: \", int( (len(set(missing_ids)) / df_email_csv[\"message_id\"].nunique()) *100 ) , \"%\"   )\n","\n","# se viene intorno al 75% va bene, perché delle email normali ne scarto 1 ogni 2, mentre di quelle group ne scarto 2 ogni 3"],"metadata":{"id":"dhp43gfIIcIw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["csv_file_path = '/content/gdrive/MyDrive/processati_email/email_460-479.csv'  # Specify the file path where you want to save the CSV\n","df_email_completo.to_csv(csv_file_path, index=False)\n","\n","print(\"DataFrame df_email_completo saved to CSV successfully!\")"],"metadata":{"id":"6VH8j5O8CHv_"},"execution_count":null,"outputs":[]}]}